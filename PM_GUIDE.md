# üìä PM Guide: Understanding False Positives in AB Testing

## üéØ What Problem Does This Solve?

When you run an AB test and track 20 metrics, some will show "statistical significance" **just by random chance**, even if your change had zero real effect. This calculator helps you understand:

1. **How many of your "wins" are real vs noise**
2. **When to use Bonferroni correction**
3. **How to set realistic expectations**

## üöÄ Quick Start (30 seconds)

1. **Enter your experiment parameters:**
   - Alpha (usually 0.05)
   - Beta (usually 0.20 for 80% power)
   - Number of metrics you're tracking
   - How many metrics you expect to move

2. **Look at the False Discovery Rate:**
   - This tells you what % of your significant results are likely false positives
   - < 10% = Good ‚úÖ
   - 10-30% = Be cautious ‚ö†Ô∏è
   - > 30% = High risk of false positives ‚ùå

3. **Toggle Bonferroni correction:**
   - See how it reduces false positives
   - But also reduces your power to detect real effects

## üí° Real-World Example

### Scenario: New Checkout Flow
You're testing a new checkout flow and tracking:
- Conversion rate (primary metric)
- Average order value
- Time to checkout
- Cart abandonment rate
- Return rate
- Customer satisfaction
- Mobile conversion
- Desktop conversion
- ... 20 metrics total

You expect 3-5 metrics to actually improve.

### Without This Tool:
‚ùå You see 6 metrics hit significance (p < 0.05)
‚ùå You celebrate and ship the change
‚ùå But 1-2 of those "wins" were probably just noise
‚ùå You might be shipping a change that doesn't actually work

### With This Tool:
‚úÖ Input: 20 metrics, expect 5 real effects, Œ±=0.05
‚úÖ Calculator shows: ~1 false positive expected
‚úÖ You see 6 significant metrics
‚úÖ You know ~1 is likely noise
‚úÖ You focus on the strongest signals or run a follow-up test

## üìñ Key Concepts Explained Simply

### Alpha (Œ±) - Your Significance Level
**What it is:** The bar you set for calling something "significant"
**Default:** 0.05 (5%)
**What it means:** "I'm okay with a 5% chance of a false positive on each metric"

**PM Translation:** 
- Œ± = 0.05 ‚Üí If you test 20 metrics with no real effects, you'll see ~1 false positive
- Lower Œ± (0.01) ‚Üí Fewer false positives, but might miss real effects
- Higher Œ± (0.10) ‚Üí Catch more real effects, but more false positives

### Beta (Œ≤) - Type II Error Rate
**What it is:** Your chance of missing a real effect
**Default:** 0.20 (which means 80% power)
**What it means:** "If there IS a real effect, I have an 80% chance of catching it"

**PM Translation:**
- Œ≤ = 0.20 ‚Üí 80% power ‚Üí You'll catch 4 out of 5 real effects
- Lower Œ≤ (0.10) ‚Üí 90% power ‚Üí Better, but requires more sample size
- Higher Œ≤ (0.30) ‚Üí 70% power ‚Üí Cheaper/faster, but might miss effects

### False Discovery Rate (FDR)
**What it is:** Of your significant results, what % are false positives
**Target:** < 10% is good, < 5% is great

**PM Translation:**
- FDR = 20% ‚Üí 1 in 5 of your "wins" is probably noise
- FDR = 50% ‚Üí Half your "wins" are probably noise
- FDR = 5% ‚Üí Very confident in your significant results

### Bonferroni Correction
**What it is:** Divides your alpha by the number of metrics
**Example:** Œ± = 0.05, 20 metrics ‚Üí Bonferroni Œ± = 0.0025

**PM Translation:**
- ‚úÖ Dramatically reduces false positives
- ‚ùå Also reduces power (might miss real effects)
- üí° Use when: Testing many metrics and false positives are costly
- üí° Skip when: Few metrics or need to catch every possible effect

## üéØ When to Use Bonferroni

### Use Bonferroni When:
‚úÖ Testing 10+ metrics
‚úÖ False positives are expensive (engineering time, user confusion)
‚úÖ You're doing exploratory analysis
‚úÖ Results will be used for major decisions
‚úÖ You can afford to miss some smaller effects

### Skip Bonferroni When:
‚ùå Testing 1-3 primary metrics
‚ùå You pre-registered your metrics
‚ùå You need high power to detect effects
‚ùå You'll run follow-up tests anyway
‚ùå Sample size is limited

## üìä Rule of Thumb Guidelines

### Number of Metrics
- **1-5 metrics:** Low risk, probably don't need Bonferroni
- **5-20 metrics:** Moderate risk, consider Bonferroni for secondary metrics
- **20+ metrics:** High risk, strongly consider Bonferroni or reduce metrics

### Expected Effects
- **Many expected effects (>50% of metrics):** Lower FDR, less need for correction
- **Few expected effects (<25% of metrics):** Higher FDR, more need for correction
- **No expected effects (guardrail metrics):** Very high FDR, definitely use correction

### False Discovery Rate
- **< 10%:** Good! Your significant results are mostly real
- **10-20%:** Okay, but be cautious. Consider follow-up tests
- **20-30%:** Risky. Many "wins" might be noise
- **> 30%:** Very risky. Use Bonferroni or reduce metrics

## üõ†Ô∏è Practical Workflows

### Workflow 1: Primary + Secondary Metrics
```
Primary Metrics (3-5):
- Don't apply Bonferroni
- These are your key metrics, pre-registered
- Accept standard Œ± = 0.05

Secondary Metrics (10-20):
- Apply Bonferroni
- These are exploratory
- Use adjusted Œ± to control false positives
```

### Workflow 2: Tiered Approach
```
Tier 1 (Must-move metrics): Œ± = 0.05, no correction
Tier 2 (Should-move metrics): Œ± = 0.01, no correction  
Tier 3 (Nice-to-move metrics): Œ± = 0.05, with Bonferroni
```

### Workflow 3: Sequential Testing
```
1. Run test with all metrics
2. See which are significant
3. Use this calculator to estimate false positives
4. Run follow-up test on significant metrics only
5. Only ship if significant in both tests
```

## ‚ö†Ô∏è Common Mistakes PMs Make

### 1. Testing Everything
‚ùå **Mistake:** Track 50 metrics "just in case"
‚úÖ **Better:** Pick 5-10 key metrics before the test

### 2. P-Hacking
‚ùå **Mistake:** Test 20 metrics, only report the 3 significant ones
‚úÖ **Better:** Pre-register your metrics, report all results

### 3. Ignoring Multiple Testing
‚ùå **Mistake:** "We got 5 significant results!" (out of 30 metrics tested)
‚úÖ **Better:** "We tested 30 metrics, expected ~1.5 false positives, got 5 significant"

### 4. Stopping Early
‚ùå **Mistake:** Check results daily, stop when significant
‚úÖ **Better:** Pre-define sample size, run to completion

### 5. Segment Fishing
‚ùå **Mistake:** "Not significant overall, but significant for iOS users aged 25-34!"
‚úÖ **Better:** Pre-register segments, or treat as exploratory

### 6. Changing Metrics Mid-Test
‚ùå **Mistake:** Add new metrics after seeing results
‚úÖ **Better:** Lock metrics before test starts

## üí™ Best Practices

### Before the Test
1. **Pre-register your metrics** - Decide what matters before you see data
2. **Separate primary vs secondary** - Different standards for different tiers
3. **Set sample size** - Don't peek and stop early
4. **Define success criteria** - What would make you ship?

### During the Test
5. **Don't peek** - Or if you do, don't stop early based on significance
6. **Monitor guardrails** - But don't count them in your multiple testing
7. **Document everything** - What you're testing and why

### After the Test
8. **Use this calculator** - Understand your false positive risk
9. **Consider effect sizes** - Not just p-values
10. **Run follow-up tests** - Validate surprising results
11. **Be honest about results** - Report all metrics tested, not just significant ones

## üìà Example Calculations

### Example 1: Focused Test
```
Metrics: 5
Expected effects: 3
Alpha: 0.05
Beta: 0.20

Result:
- Expected significant: 3.5
- False positives: 0.1
- FDR: 3%

Interpretation: Very low false positive risk ‚úÖ
```

### Example 2: Dashboard Test
```
Metrics: 30
Expected effects: 5
Alpha: 0.05
Beta: 0.20

Result:
- Expected significant: 5.25
- False positives: 1.25
- FDR: 24%

Interpretation: ~1 in 4 "wins" is noise ‚ö†Ô∏è
Consider Bonferroni!
```

### Example 3: With Bonferroni
```
Same as Example 2, but with Bonferroni:

Result:
- Adjusted alpha: 0.0017
- Expected significant: 4.04
- False positives: 0.04
- FDR: 1%

Interpretation: Much lower false positives ‚úÖ
But also detecting fewer real effects (4 vs 5)
```

## üéì Learning Resources

### For PMs
- [Evan Miller's AB Testing Tools](https://www.evanmiller.org/ab-testing/)
- [Optimizely's Stats Engine](https://www.optimizely.com/optimization-glossary/statistical-significance/)

### For Data Scientists
- [Multiple Comparisons Problem (Wikipedia)](https://en.wikipedia.org/wiki/Multiple_comparisons_problem)
- [False Discovery Rate (Benjamini-Hochberg)](https://en.wikipedia.org/wiki/False_discovery_rate)

## ü§ù Working with Data Scientists

### Questions to Ask Your DS Team
1. "How many metrics are we testing?"
2. "What's our expected false discovery rate?"
3. "Should we use multiple testing correction?"
4. "How many of these significant results are likely real?"
5. "What's our statistical power?"

### Information to Provide
1. Which metrics are primary vs exploratory
2. What effect sizes matter for business
3. Cost of false positives vs false negatives
4. Whether you'll run follow-up tests

## üìû Need Help?

- Use the calculator to run scenarios
- Consult with your data science team
- When in doubt, be conservative (use Bonferroni)
- Consider follow-up tests for surprising results

---

**Remember:** Statistical significance ‚â† Real effect. Always consider:
- Effect size (how big is the change?)
- Business impact (does it matter?)
- Confidence (how many metrics did we test?)
- Replication (can we reproduce it?)

Happy testing! üìä

